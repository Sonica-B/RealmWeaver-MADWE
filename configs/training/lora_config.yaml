# LoRA Training Configuration for MADWE

# Model configuration
model:
  base_model_id: "stabilityai/stable-diffusion-xl-base-1.0"
  use_safetensors: true
  mixed_precision: true  # fp16 training
  dtype: "float16"

# LoRA configuration
lora:
  rank: 8
  alpha: 8  # Usually same as rank
  dropout: 0.1
  target_modules:
    - "to_k"
    - "to_q" 
    - "to_v"
    - "to_out.0"

# Training configuration
training:
  batch_size: 1
  gradient_accumulation_steps: 4  # Effective batch size = 4
  learning_rate: 1e-4
  weight_decay: 0.01
  num_epochs: 10
  num_train_steps: 10000
  warmup_steps: 500
  max_grad_norm: 1.0
  
  # Optimizer
  optimizer_type: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  
  # Scheduler
  lr_scheduler: "cosine"
  min_lr_ratio: 0.1

# Loss configuration
loss:
  use_style_loss: true
  style_loss_weight: 0.1
  use_perceptual_loss: false
  perceptual_loss_weight: 0.05

# Data configuration
data:
  resolution: 512
  num_workers: 4
  augmentation:
    random_flip: true
    color_jitter: true
    brightness: 0.1
    contrast: 0.1

# Logging & Checkpointing
logging:
  log_every: 50
  val_every: 1000
  save_every: 500
  log_with: "tensorboard"
  
# Hardware configuration  
hardware:
  mixed_precision: "fp16"
  enable_xformers: true
  enable_gradient_checkpointing: true
  
# Biome-specific overrides
biomes:
  forest:
    style_loss_weight: 0.15
    learning_rate: 8e-5
  desert:
    style_loss_weight: 0.1
    learning_rate: 1e-4
  cyberpunk:
    style_loss_weight: 0.2
    learning_rate: 1.2e-4
  underwater:
    style_loss_weight: 0.12
    learning_rate: 9e-5